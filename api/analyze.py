import json
import asyncio
import os
import gc
import re
from typing import List, Dict, Optional, Any
from flask import Blueprint, request, jsonify
import aiohttp
from datetime import datetime, timezone, timedelta
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from threading import Lock

# --- Google API imports (chá»‰ cho Gmail vÃ  Sheets) ---
import base64
from email.mime.text import MIMEText
from googleapiclient.discovery import build
from google.oauth2.credentials import Credentials

# --- Blueprint ---
analyze_endpoint = Blueprint('analyze_endpoint', __name__)

# --- Cáº¥u hÃ¬nh ---
SAFE_BROWSING_API_KEY = os.environ.get('SAFE_BROWSING_API_KEY')
GMAIL_TOKEN_PATH = os.environ.get('GMAIL_TOKEN_PATH')
GOOGLE_SHEET_ID = os.environ.get('GOOGLE_SHEET_ID')

# --- Cáº¥u hÃ¬nh Gemma Model ---
MODEL_NAME = "google/gemma-3-270m-it"
CACHE_DIR = os.path.join(os.getcwd(), "models", "gemma270m")

# --- Sheet names ---
DANGEROUS_SHEET_NAME = "DangerousPatterns"
HINT_SHEET_NAME = "SafePatterns"
TRIVIAL_SHEET_NAME = "TrivialPatterns"

# --- Cache configuration ---
cached_dangerous_regex = None
cached_trivial_set = None
cache_timestamp = 0
CACHE_DURATION = 1200000  # 20 phÃºt (milliseconds)

# --- Model loading status (sá»­ dá»¥ng class Ä‘á»ƒ Ä‘áº£m báº£o state Ä‘Æ°á»£c share) ---
class ModelState:
    """Singleton class Ä‘á»ƒ quáº£n lÃ½ tráº¡ng thÃ¡i model"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.model = None
            cls._instance.tokenizer = None
            cls._instance.model_lock = Lock()
            cls._instance.loaded = False
        return cls._instance
    
    def is_loaded(self):
        return self.loaded and self.model is not None and self.tokenizer is not None

# Khá»Ÿi táº¡o singleton
model_state = ModelState()

# =================================================================
# GEMMA MODEL INITIALIZATION
# =================================================================

def load_gemma_model():
    """Táº£i Gemma model vÃ o memory má»™t láº§n duy nháº¥t"""
    global model_state
    
    if model_state.is_loaded():
        print("âœ… [Gemma] Model Ä‘Ã£ Ä‘Æ°á»£c táº£i trÆ°á»›c Ä‘Ã³")
        return True
    
    try:
        print(f"â³ [Gemma] Äang táº£i Gemma-3-270M-IT tá»« Hugging Face...")
        print(f"ğŸ“ [Gemma] Cache directory: {CACHE_DIR}")
        
        # Táº£i tokenizer
        model_state.tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            cache_dir=CACHE_DIR,
            #torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # Táº£i model vá»›i tá»‘i Æ°u hÃ³a
        model_state.model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            cache_dir=CACHE_DIR,
            #torch_dtype=torch.float16,  # Sá»­ dá»¥ng float16 Ä‘á»ƒ tiáº¿t kiá»‡m memory
            device_map="auto",  # Tá»± Ä‘á»™ng phÃ¢n bá»• lÃªn GPU náº¿u cÃ³
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # Set to eval mode
        model_state.model.eval()
        
        model_state.loaded = True
        device = next(model_state.model.parameters()).device
        print(f"âœ… [Gemma] Model Ä‘Ã£ táº£i xong vÃ  sáºµn sÃ ng trÃªn {device}!")
        
        # Test nhanh
        test_prompt = "Xin chÃ o"
        test_result = generate_with_gemma(test_prompt, max_tokens=10)
        print(f"ğŸ§ª [Gemma] Test inference: '{test_result[:50]}...'")
        
        return True
        
    except Exception as e:
        print(f"ğŸ”´ [Gemma] Lá»—i khi táº£i model: {e}")
        model_state.loaded = False
        return False

def generate_with_gemma(prompt: str, max_tokens: int = 100, temperature: float = 0.2) -> str:
    """
    Generate text vá»›i Gemma model
    Args:
        prompt: Input prompt
        max_tokens: Sá»‘ token tá»‘i Ä‘a Ä‘á»ƒ generate
        temperature: Äá»™ sÃ¡ng táº¡o (0.0 = deterministic, 1.0 = creative)
    Returns:
        Generated text (chá»‰ pháº§n má»›i, khÃ´ng bao gá»“m prompt)
    """
    if not model_state.is_loaded():
        print("ğŸ”´ [Gemma] Model chÆ°a Ä‘Æ°á»£c táº£i!")
        return ""
    
    try:
        with model_state.model_lock:  # Thread-safe inference
            # Tokenize input
            inputs = model_state.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
            inputs = {k: v.to(model_state.model.device) for k, v in inputs.items()}
            
            # Generate vá»›i torch.no_grad() Ä‘á»ƒ tiáº¿t kiá»‡m memory
            with torch.no_grad():
                outputs = model_state.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.9,
                    do_sample=temperature > 0,
                    pad_token_id=model_state.tokenizer.eos_token_id
                )
            
            # Decode output
            full_text = model_state.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Tráº£ vá» chá»‰ pháº§n generated (bá» prompt)
            generated_text = full_text[len(prompt):].strip()
            
            return generated_text
            
    except Exception as e:
        print(f"ğŸ”´ [Gemma] Lá»—i khi generate: {e}")
        return ""

def generate_json_with_gemma(prompt: str, max_tokens: int = 200) -> dict:
    """
    Generate JSON response vá»›i Gemma
    Cá»‘ gáº¯ng parse JSON tá»« output, náº¿u fail thÃ¬ tráº£ vá» error
    """
    try:
        raw_output = generate_with_gemma(prompt, max_tokens=max_tokens, temperature=0.1)
        
        # TÃ¬m JSON trong output (cÃ³ thá»ƒ model tráº£ vá» text + JSON)
        json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            return json.loads(json_str)
        else:
            # Thá»­ parse trá»±c tiáº¿p
            return json.loads(raw_output)
            
    except json.JSONDecodeError as e:
        print(f"ğŸŸ¡ [Gemma] KhÃ´ng parse Ä‘Æ°á»£c JSON: {e}")
        print(f"Raw output: {raw_output[:200]}")
        return {"error": "Invalid JSON response", "raw": raw_output[:100]}
    except Exception as e:
        print(f"ğŸ”´ [Gemma] Lá»—i khi generate JSON: {e}")
        return {"error": str(e)}

# =================================================================
# GOOGLE API CREDENTIALS & SERVICES
# =================================================================

def get_google_credentials(scopes):
    """Láº¥y credentials cho Google API vá»›i cÃ¡c scope cáº§n thiáº¿t."""
    if not os.path.exists(GMAIL_TOKEN_PATH):
        print(f"ğŸ”´ [Google API] Lá»—i: KhÃ´ng tÃ¬m tháº¥y tá»‡p token táº¡i '{GMAIL_TOKEN_PATH}'")
        return None
    try:
        return Credentials.from_authorized_user_file(GMAIL_TOKEN_PATH, scopes)
    except Exception as e:
        print(f"ğŸ”´ [Google API] Lá»—i khi táº£i credentials: {e}")
        return None

def send_email_gmail_api(to_email, subject, body):
    """Gá»­i email qua Gmail API"""
    creds = get_google_credentials(['https://www.googleapis.com/auth/gmail.send'])
    if not creds:
        print("ğŸ”´ [Email] KhÃ´ng thá»ƒ gá»­i email do lá»—i credentials.")
        return
    service = build('gmail', 'v1', credentials=creds)
    message = MIMEText(body)
    message['to'] = to_email
    message['subject'] = subject
    raw = base64.urlsafe_b64encode(message.as_bytes()).decode()
    result = service.users().messages().send(userId='me', body={'raw': raw}).execute()
    return result

async def save_to_history_sheet_async(text: str, result: dict):
    """LÆ°u káº¿t quáº£ phÃ¢n tÃ­ch vÃ o Google Sheet má»™t cÃ¡ch báº¥t Ä‘á»“ng bá»™."""
    print("â¡ï¸ [Sheet] Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh lÆ°u lá»‹ch sá»­...")
    if not GOOGLE_SHEET_ID:
        print("ğŸ”´ [Sheet] Lá»—i: Biáº¿n mÃ´i trÆ°á»ng GOOGLE_SHEET_ID chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p.")
        return

    creds = get_google_credentials(['https://www.googleapis.com/auth/spreadsheets'])
    if not creds:
        print("ğŸ”´ [Sheet] KhÃ´ng thá»ƒ lÆ°u vÃ o Sheet do lá»—i credentials.")
        return

    try:
        service = build('sheets', 'v4', credentials=creds)
        
        # Láº¥y thá»i gian hiá»‡n táº¡i, mÃºi giá» Viá»‡t Nam (UTC+7)
        vn_timezone = timezone(timedelta(hours=7))
        timestamp = datetime.now(vn_timezone).strftime('%Y-%m-%d %H:%M:%S')

        # Chuáº©n bá»‹ dá»¯ liá»‡u hÃ ng
        row_data = [
            timestamp,
            text,
            result.get('is_dangerous', False),
            result.get('types', 'N/A'),
            result.get('reason', 'N/A'),
            result.get('score', 0),
            result.get('recommend', 'N/A')
        ]

        body = {'values': [row_data]}
        
        # Gá»­i yÃªu cáº§u append
        sheet_result = service.spreadsheets().values().append(
            spreadsheetId=GOOGLE_SHEET_ID,
            range='History!A2',
            valueInputOption='USER_ENTERED',
            insertDataOption='INSERT_ROWS',
            body=body
        ).execute()
        
        print(f"âœ… [Sheet] ÄÃ£ lÆ°u thÃ nh cÃ´ng vÃ o Google Sheet: {sheet_result.get('updates').get('updatedRange')}")

    except Exception as e:
        print(f"ğŸ”´ [Sheet] Lá»—i khi Ä‘ang lÆ°u vÃ o Google Sheet: {e}")

# =================================================================
# GOOGLE SHEETS DATA ACCESS
# =================================================================

def get_sheet_data(sheet_name: str) -> Optional[List[Dict]]:
    """Láº¥y dá»¯ liá»‡u tá»« Google Sheet"""
    if not GOOGLE_SHEET_ID:
        return None
    
    try:
        creds = get_google_credentials(['https://www.googleapis.com/auth/spreadsheets.readonly'])
        if not creds:
            return None
            
        service = build('sheets', 'v4', credentials=creds)
        result = service.spreadsheets().values().get(
            spreadsheetId=GOOGLE_SHEET_ID,
            range=f'{sheet_name}!A:Z'
        ).execute()
        
        values = result.get('values', [])
        if len(values) < 2:
            return []
        
        headers = values[0]
        data_rows = values[1:]
        
        parsed_data = []
        for idx, row in enumerate(data_rows):
            row_data = {'id': idx}
            for i, header in enumerate(headers):
                value = row[i] if i < len(row) else ''
                
                if header == 'text':
                    value = str(value or "").strip().lower()
                elif header == 'is_dangerous':
                    value = str(value).lower() == 'true'
                elif header == 'score':
                    value = int(value) if value else 0
                
                row_data[header] = value
            
            if row_data.get('text'):  # Chá»‰ thÃªm náº¿u cÃ³ text
                parsed_data.append(row_data)
        
        return parsed_data
        
    except Exception as e:
        print(f"ğŸ”´ [Sheet] KhÃ´ng thá»ƒ Ä‘á»c sheet {sheet_name}: {e}")
        return None

# =================================================================
# LEO ENGINE - PATTERN MATCHING & LOCAL AI DATABASE
# =================================================================

def is_trivial_by_pattern(input_text: str) -> bool:
    """Kiá»ƒm tra xem tin nháº¯n cÃ³ pháº£i lÃ  trivial khÃ´ng báº±ng pattern matching"""
    text = input_text.strip()
    
    # Chá»‰ emoji/kaomoji/sticker
    emoji_pattern = r'^[\s\p{Emoji}\p{Emoji_Component}():><\-_.,;!?*\'"+/\\|@#$%^&{}[\]~`]*$'
    try:
        if re.match(emoji_pattern, text):
            return True
    except:
        pass
    
    # Tá»« duy nháº¥t, quÃ¡ ngáº¯n (<4 kÃ½ tá»±)
    words = text.split()
    if len(words) == 1 and len(text) <= 3:
        return True
    
    # Common trivial Vietnamese words
    trivial_words = ['uk', 'hÃ³ng', 'Ã ', 'Ã¡', 'j', 'hm', 'hmm', 'huhu', 'he he', 'oke', 'ok', 'vÃ¢ng', 'dáº¡']
    if text.lower() in trivial_words:
        return True
    
    return False

def is_trivial_direct(input_normalized: str, trivial_sheet_data: List[Dict]) -> bool:
    """Kiá»ƒm tra trivial báº±ng direct match"""
    global cached_trivial_set
    
    if not trivial_sheet_data:
        return False
    
    if cached_trivial_set is None:
        cached_trivial_set = set()
        for item in trivial_sheet_data:
            normalized = str(item.get('text', '')).lower().strip()
            if normalized:
                cached_trivial_set.add(normalized)
    
    return input_normalized in cached_trivial_set

def is_trivial_message_local_ai(input_text: str) -> bool:
    """Sá»­ dá»¥ng Gemma LOCAL AI Ä‘á»ƒ kiá»ƒm tra xem tin nháº¯n cÃ³ trivial khÃ´ng"""
    if len(input_text) > 100:
        return False
    
    # Thá»­ pattern trÆ°á»›c
    if is_trivial_by_pattern(input_text):
        print("âœ… [Tiá»ƒu AI] Tin nháº¯n lÃ  táº§m thÆ°á»ng (pattern match)")
        return True
    
    prompt = f"""PhÃ¢n tÃ­ch tin nháº¯n: "{input_text}"

Tin nháº¯n nÃ y cÃ³ thuá»™c má»™t trong cÃ¡c loáº¡i sau khÃ´ng:
1. Chá»‰ chá»©a emoji hoáº·c biá»ƒu tÆ°á»£ng cáº£m xÃºc
2. Má»™t tá»« vÃ´ nghÄ©a khÃ´ng cÃ³ ngá»¯ cáº£nh (vÃ­ dá»¥: "uk", "hÃ³ng")
3. CÃ¢u há»i cá»¥t lá»§n khÃ´ng cÃ³ ná»™i dung
4. Chá»‰ chá»©a dáº¥u cÃ¢u

Tráº£ lá»i JSON: {{"result": true}} náº¿u Táº¦M THÆ¯á»œNG, {{"result": false}} náº¿u Cáº¦N PHÃ‚N TÃCH.
Chá»‰ tráº£ vá» JSON, khÃ´ng giáº£i thÃ­ch."""

    result = generate_json_with_gemma(prompt, max_tokens=30)
    
    if 'error' not in result:
        is_trivial = result.get('result', False) == True
        print(f"âœ… [Tiá»ƒu AI] Gemma phÃ¢n tÃ­ch: '{input_text}' táº§m thÆ°á»ng? -> {is_trivial}")
        return is_trivial
    
    return False

def escape_regex(text: str) -> str:
    """Escape cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t trong regex"""
    return re.escape(str(text or ""))

def get_dangerous_regex(dangerous_sheet_data: List[Dict]):
    """Láº¥y hoáº·c xÃ¢y dá»±ng cached regex tá»« dangerous patterns"""
    global cached_dangerous_regex, cache_timestamp
    
    now = datetime.now().timestamp() * 1000  # milliseconds
    
    # Kiá»ƒm tra cache cÃ²n háº¡n khÃ´ng
    if cached_dangerous_regex and (now - cache_timestamp) < CACHE_DURATION:
        return cached_dangerous_regex
    
    if not dangerous_sheet_data:
        return None
    
    try:
        escaped_patterns = [escape_regex(item.get('text', '')) for item in dangerous_sheet_data if item.get('text')]
        
        if not escaped_patterns:
            return None
        
        regex_pattern = '|'.join(escaped_patterns)
        cached_dangerous_regex = re.compile(regex_pattern, re.IGNORECASE)
        cache_timestamp = now
        
        print(f"âœ… [Cache] Built dangerous regex with {len(escaped_patterns)} patterns")
        return cached_dangerous_regex
    except Exception as e:
        print(f"ğŸ”´ [Cache] Error building regex: {e}")
        return None

def call_smart_db_ai_local(input_text: str, known_data: List[Dict]) -> int:
    """Gá»i LOCAL Gemma AI Ä‘á»ƒ semantic search trong dangerous patterns"""
    
    # Giá»›i háº¡n sá»‘ patterns Ä‘á»ƒ khÃ´ng vÆ°á»£t quÃ¡ context window
    max_patterns = 50
    limited_data = known_data[:max_patterns]
    
    known_texts_str = "\n".join([f"ID {item['id']}: \"{item.get('text', '')}\"" for item in limited_data])
    
    prompt = f"""Báº¡n lÃ  cá»— mÃ¡y tÃ¬m kiáº¿m ngá»¯ nghÄ©a.

CÆ  Sá» Dá»® LIá»†U:
{known_texts_str}

TIN NHáº®N: "{input_text}"

NHIá»†M Vá»¤: TÃ¬m máº«u cÃ³ Ã½ nghÄ©a TÆ¯Æ NG Äá»’NG (>95% cháº¯c cháº¯n).
- Náº¿u tÃ¬m tháº¥y: tráº£ vá» CHá»ˆ Sá» ID cá»§a máº«u
- Náº¿u khÃ´ng: tráº£ vá» -1

Chá»‰ tráº£ vá» má»™t sá»‘ duy nháº¥t, khÃ´ng giáº£i thÃ­ch."""

    result_text = generate_with_gemma(prompt, max_tokens=20, temperature=0.0)
    
    # Parse sá»‘ tá»« output
    try:
        # TÃ¬m sá»‘ trong output
        match = re.search(r'-?\d+', result_text)
        if match:
            match_id = int(match.group())
            print(f"âœ… [DB-AI] Gemma Decision ID: {match_id}")
            return match_id
    except:
        pass
    
    return -1

async def leo_db_engine(text: str) -> Dict[str, Any]:
    """
    LEO DATABASE ENGINE - Integrated pattern matching vÃ  LOCAL AI database
    Returns: {"found": bool, "type": str, "data": dict, "source": str, "confidence": str}
    """
    input_normalized = text.lower().strip()
    
    # === BÆ¯á»šC 1: TRIVIAL - Direct Match ===
    trivial_data = get_sheet_data(TRIVIAL_SHEET_NAME)
    if trivial_data and is_trivial_direct(input_normalized, trivial_data):
        print("âœ… [Leo] Found in TrivialPatterns (direct)")
        return {
            "found": True,
            "type": "trivial_pattern",
            "source": "direct_match",
            "confidence": "high"
        }
    
    # === BÆ¯á»šC 2: DANGEROUS - Direct Match ===
    dangerous_data = get_sheet_data(DANGEROUS_SHEET_NAME)
    if dangerous_data:
        direct_match = next((item for item in dangerous_data if item.get('text') == input_normalized), None)
        if direct_match:
            print(f"âœ… [Leo] Exact match in DangerousPatterns ID: {direct_match['id']}")
            return {
                "found": True,
                "type": "dangerous_pattern",
                "data": direct_match,
                "source": "direct_match",
                "confidence": "high"
            }
    
    # === BÆ¯á»šC 3: DANGEROUS - Regex Match ===
    if dangerous_data:
        regex = get_dangerous_regex(dangerous_data)
        if regex and regex.search(input_normalized):
            print("âœ… [Leo] Regex match in DangerousPatterns")
            matched_item = next((item for item in dangerous_data 
                               if re.search(escape_regex(item.get('text', '')), input_normalized, re.IGNORECASE)), None)
            if matched_item:
                return {
                    "found": True,
                    "type": "dangerous_pattern",
                    "data": matched_item,
                    "source": "regex_match",
                    "confidence": "medium"
                }
    
    # === BÆ¯á»šC 4: TRIVIAL - Local AI Check ===
    if len(text) <= 100:
        if is_trivial_message_local_ai(text):
            print("âœ… [Leo] Gemma xÃ¡c Ä‘á»‹nh táº§m thÆ°á»ng")
            return {
                "found": True,
                "type": "trivial_pattern",
                "source": "local_ai_check",
                "confidence": "medium"
            }
    
    # === BÆ¯á»šC 5: DANGEROUS - Local AI Semantic Search ===
    if dangerous_data and len(dangerous_data) > 0:
        match_id = call_smart_db_ai_local(input_normalized, dangerous_data)
        if match_id != -1:
            matched_item = next((item for item in dangerous_data if item['id'] == match_id), None)
            if matched_item:
                print(f"âœ… [Leo] Gemma Match ID: {match_id}")
                return {
                    "found": True,
                    "type": "dangerous_pattern",
                    "data": matched_item,
                    "source": "local_ai_semantic",
                    "confidence": "high"
                }
    
    # === BÆ¯á»šC 6: CONTEXT HINTS ===
    hint_data = get_sheet_data(HINT_SHEET_NAME)
    if hint_data:
        for item in hint_data:
            keyword = str(item.get('text', '')).strip().lower()
            hint = item.get('hint', item.get('types', ''))
            if keyword and hint and keyword in input_normalized:
                print(f"âœ… [Leo] Context hint: {keyword}")
                return {
                    "found": True,
                    "type": "context_hint",
                    "data": hint,
                    "source": "keyword_match",
                    "confidence": "low"
                }
    
    # === BÆ¯á»šC 7: KHÃ”NG CÃ“ GÃŒ KHá»šP ===
    print("â„¹ï¸ [Leo] No match found")
    return {
        "found": False,
        "reason": "No patterns or hints found."
    }

# =================================================================
# URL SAFETY CHECK
# =================================================================

async def check_urls_safety_optimized(urls: list):
    """Kiá»ƒm tra URL vá»›i Google Safe Browsing API"""
    if not SAFE_BROWSING_API_KEY or not urls:
        return []
    
    print("â¡ï¸ [URL Check] Checking with Google Safe Browsing...")
    safe_browsing_url = f"https://safebrowsing.googleapis.com/v4/threatMatches:find?key={SAFE_BROWSING_API_KEY}"
    payload = {
        "threatInfo": {
            "threatTypes": ["MALWARE", "SOCIAL_ENGINEERING"],
            "platformTypes": ["ANY_PLATFORM"],
            "threatEntryTypes": ["URL"],
            "threatEntries": [{"url": url} for url in urls[:5]]
        }
    }
    
    try:
        timeout = aiohttp.ClientTimeout(total=15)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(safe_browsing_url, json=payload) as resp:
                if resp.status == 200:
                    matches = (await resp.json()).get("matches", [])
                    print(f"âœ… [URL Check] Found {len(matches)} unsafe URLs")
                    return matches
                print(f"ğŸŸ¡ [URL Check] API status {resp.status}")
                return []
    except Exception as e:
        print(f"ğŸ”´ [URL Check] Failed: {e}")
        return []

# =================================================================
# ANNA-AI ANALYSIS (LOCAL GEMMA)
# =================================================================

def create_anna_ai_prompt(text: str, context_hint: str = None):
    """Táº¡o prompt cho Anna-AI vá»›i Gemma"""
    hint_section = ""
    if context_hint:
        hint_section = f"""
THÃ”NG TIN TÃŒNH BÃO Bá»” SUNG:
"{context_hint}"
---
"""
    
    return f"""Báº¡n lÃ  Anna, chuyÃªn gia an ninh máº¡ng phÃ¢n tÃ­ch tin nháº¯n tiáº¿ng Viá»‡t.

Má»¤C TIÃŠU: Báº£o vá»‡ ngÆ°á»i dÃ¹ng khá»i cÃ¡c má»‘i Ä‘e dá»a rÃµ rÃ ng:
- Lá»«a Ä‘áº£o / phishing
- Báº¡o lá»±c há»c Ä‘Æ°á»ng / Ä‘e dá»a
- NgÃ´n tá»« thÃ¹ ghÃ©t / kÃ­ch Ä‘á»™ng
- TuyÃªn truyá»n chá»‘ng phÃ¡
- HÃ nh vi gÃ¢y háº¡i khÃ¡c

QUY Táº®C VÃ€NG: Máº·c Ä‘á»‹nh Má»ŒI tin nháº¯n lÃ  AN TOÃ€N trá»« khi cÃ³ báº±ng chá»©ng rÃµ rÃ ng vá» Ã½ Ä‘á»“ xáº¥u VÃ€ hÃ nh Ä‘á»™ng gÃ¢y háº¡i.

{hint_section}

PHÃ‚N TÃCH 3 BÆ¯á»šC:

1. Lá»ŒC NHIá»„U:
- Tin nháº¯n quÃ¡ ngáº¯n (<4 tá»«) hoáº·c chá»‰ emoji â†’ AN TOÃ€N ngay

2. NGá»® Cáº¢NH & Ã Äá»ŠNH:
- Máº·c Ä‘á»‹nh: báº¡n bÃ¨ trÃ² chuyá»‡n bÃ¬nh thÆ°á»ng
- Chá»‰ Ä‘Ã¡nh dáº¥u NGUY HIá»‚M náº¿u kÃªu gá»i hÃ nh Ä‘á»™ng cá»¥ thá»ƒ gÃ¢y háº¡i

3. Káº¾T LUáº¬N Dá»°A TRÃŠN Báº°NG CHá»¨NG:
- NGUY HIá»‚M: Ã½ Ä‘á»“ xáº¥u RÃ• RÃ€NG + hÃ nh Ä‘á»™ng gÃ¢y háº¡i Cá»¤ THá»‚
- AN TOÃ€N: táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p khÃ¡c

TIN NHáº®N: "{text}"

Tráº£ vá» JSON (tiáº¿ng Viá»‡t):
{{
  "is_dangerous": boolean,
  "reason": "giáº£i thÃ­ch ngáº¯n gá»n",
  "types": "scam|violence|hate_speech|anti_state|other",
  "score": 0-5,
  "recommend": "khuyáº¿n nghá»‹"
}}

Chá»‰ tráº£ vá» JSON, khÃ´ng giáº£i thÃ­ch thÃªm."""

def analyze_with_anna_ai_local(text: str, context_hint: str = None) -> dict:
    """PhÃ¢n tÃ­ch tin nháº¯n vá»›i LOCAL Gemma Anna-AI"""
    
    prompt = create_anna_ai_prompt(text[:2000], context_hint)
    
    print(f"â¡ï¸ [Anna] Analyzing with local Gemma...")
    result = generate_json_with_gemma(prompt, max_tokens=300)
    
    if 'error' in result:
        print(f"ğŸ”´ [Anna] Analysis failed: {result.get('error')}")
        return {
            'error': 'LOCAL_AI_ERROR',
            'message': 'Gemma analysis failed',
            'status_code': 500
        }
    
    print("âœ… [Anna] Analysis complete")
    return result

# =================================================================
# MAIN ANALYSIS ORCHESTRATION
# =================================================================

async def perform_full_analysis(text: str, urls: list):
    """HÃ m Ä‘iá»u phá»‘i chÃ­nh - káº¿t há»£p Leo Engine vÃ  Anna-AI (cáº£ 2 Ä‘á»u dÃ¹ng LOCAL Gemma)"""
    final_result = None
    is_new_case_by_anna = False
    context_hint_from_leo = None
    
    print(f"ğŸ“œ [Start] Analyzing: '{text[:400]}'")
    print("â¡ï¸ [Flow 1] Calling Leo Engine (Local Gemma DB-AI)...")
    
    # Gá»i Leo Engine (local)
    leo_result = await leo_db_engine(text)
    
    if leo_result and leo_result.get("found"):
        result_type = leo_result.get("type")
        if result_type == "trivial_pattern":
            print("âœ… [Flow 1] SUCCESS - Trivial message")
            return {
                'is_dangerous': False,
                'reason': 'Tin nháº¯n quÃ¡ Ä‘Æ¡n giáº£n Ä‘á»ƒ phÃ¢n tÃ­ch.',
                'score': 0,
                'types': 'Trivial'
            }
        elif result_type == "dangerous_pattern":
            print("âœ… [Flow 1] SUCCESS - Found in Blacklist")
            final_result = leo_result.get("data")
        elif result_type == "context_hint":
            print("ğŸ“ [Flow 1] Received context hint from Leo")
            context_hint_from_leo = leo_result.get("data")
    
    if final_result is None:
        if context_hint_from_leo:
            print(f"ğŸŸ¡ [Flow 2] Calling Anna-AI with hint: '{context_hint_from_leo}'")
        else:
            print(f"ğŸŸ¡ [Flow 2] Calling Anna-AI (no hint)")
        
        final_result = analyze_with_anna_ai_local(text, context_hint_from_leo)
        print(f"ğŸ“„ [Anna Result] {json.dumps(final_result, ensure_ascii=False)}")
        
        if 'error' in final_result:
            return final_result
        is_new_case_by_anna = True
    
    # Kiá»ƒm tra URLs náº¿u cÃ³
    if urls:
        url_matches = await check_urls_safety_optimized(urls)
        if url_matches:
            print(f"âš ï¸ [URL Analysis] Found {len(url_matches)} unsafe URLs!")
            final_result.update({
                'url_analysis': url_matches,
                'is_dangerous': True,
                'score': max(final_result.get('score', 0), 4),
                'reason': (final_result.get('reason', '') + " + CÃ¡c URL khÃ´ng an toÃ n")[:100]
            })
    
    # Gá»­i email cáº£nh bÃ¡o náº¿u lÃ  trÆ°á»ng há»£p nguy hiá»ƒm má»›i
    if is_new_case_by_anna and final_result.get("is_dangerous"):
        print("â¡ï¸ [Alert] New dangerous case detected. Sending email...")
        try:
            send_email_gmail_api(
                to_email="duongpham18210@gmail.com",
                subject=f"[CyberShield Alert] Nguy hiá»ƒm má»›i: {final_result.get('types', 'Unknown')} (Score: {final_result.get('score', 'N/A')})",
                body=f"""Má»™t tin nháº¯n má»›i Ä‘Ã£ Ä‘Æ°á»£c Anna-AI (Local Gemma) phÃ¢n tÃ­ch vÃ  gáº¯n cá» NGUY HIá»‚M.
Vui lÃ²ng xem xÃ©t vÃ  bá»• sung vÃ o Google Sheets.
----------------------------------------------------------
TIN NHáº®N Gá»C:
{text}
----------------------------------------------------------
Káº¾T QUáº¢ PHÃ‚N TÃCH:
{json.dumps(final_result, indent=2, ensure_ascii=False)}
----------------------------------------------------------
Source: Local Gemma-3-270M Model
"""
            )
            print("âœ… [Email] Alert sent successfully")
        except Exception as e:
            print(f"ğŸ”´ [Email] Failed to send alert: {e}")
    
    gc.collect()
    print(f"ğŸ [End] Analysis complete: '{text[:50]}...'")
    return final_result

# =================================================================
# FLASK ENDPOINTS
# =================================================================

@analyze_endpoint.route('/analyze', methods=['POST'])
async def analyze_text():
    """Endpoint chÃ­nh Ä‘á»ƒ phÃ¢n tÃ­ch tin nháº¯n"""
    try:
        print(f"ğŸ” [Debug] model_state.is_loaded() = {model_state.is_loaded()}")
        
        # Kiá»ƒm tra model Ä‘Ã£ load chÆ°a
        if not model_state.is_loaded():
            print("ğŸ”´ [API] Model chÆ°a sáºµn sÃ ng!")
            return jsonify({
                'error': 'Model chÆ°a sáºµn sÃ ng',
                'message': 'Há»‡ thá»‘ng Ä‘ang khá»Ÿi Ä‘á»™ng. Vui lÃ²ng thá»­ láº¡i sau vÃ i giÃ¢y.',
                'code': 'MODEL_NOT_READY'
            }), 503
        
        data = request.get_json(silent=True)
        if not data or 'text' not in data:
            return jsonify({'error': 'Äá»‹nh dáº¡ng yÃªu cáº§u khÃ´ng há»£p lá»‡'}), 400
        
        text = data.get('text', '').strip()
        urls = data.get('urls', [])
        
        print(f"--------------------\nğŸ“¬ [Input] Message received: '{text[:1000]}...'")
        if not text:
            return jsonify({'error': 'KhÃ´ng cÃ³ vÄƒn báº£n Ä‘á»ƒ phÃ¢n tÃ­ch'}), 400
        
        # Thá»±c hiá»‡n phÃ¢n tÃ­ch
        result = await perform_full_analysis(text[:3000], urls)
        
        # Xá»­ lÃ½ lá»—i
        if 'error' in result:
            return jsonify({'error': result.get('message', 'Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh')}), result.get('status_code', 500)
        
        # Gá»­i pháº£n há»“i cho client ngay láº­p tá»©c
        response = jsonify({'result': result})
        
        # Sau khi cÃ³ pháº£n há»“i, táº¡o má»™t tÃ¡c vá»¥ ná»n Ä‘á»ƒ lÆ°u vÃ o sheet
        asyncio.create_task(save_to_history_sheet_async(text, result))
        
        print("âœ… [Response] Result sent to client. Background save scheduled.")
        return response
        
    except Exception as e:
        import traceback
        print(f"ğŸ”´ [CRITICAL ERROR] Server error in analyze_text: {e}")
        print(f"ğŸ”´ [TRACEBACK] {traceback.format_exc()}")
        gc.collect()
        return jsonify({'error': 'Lá»—i ná»™i bá»™ server', 'details': str(e)}), 500

@analyze_endpoint.route('/health', methods=['GET'])
async def health_check():
    """Endpoint kiá»ƒm tra sá»©c khá»e cá»§a há»‡ thá»‘ng"""
    cache_info = {
        'dangerous_regex_cached': cached_dangerous_regex is not None,
        'trivial_set_cached': cached_trivial_set is not None,
        'cache_age_ms': (datetime.now().timestamp() * 1000) - cache_timestamp if cache_timestamp > 0 else 0
    }
    
    model_info = {
        'loaded': model_state.is_loaded(),
        'model_name': MODEL_NAME if model_state.is_loaded() else None,
        'device': str(next(model_state.model.parameters()).device) if model_state.is_loaded() else None,
        'dtype': str(next(model_state.model.parameters()).dtype) if model_state.is_loaded() else None
    }
    
    return jsonify({
        'status': 'healthy' if model_state.is_loaded() else 'model_not_loaded',
        'architecture': 'Leo Engine (Local Gemma DB-AI) + Anna-AI (Local Gemma)',
        'model_info': model_info,
        'cache_info': cache_info,
        'components': {
            'leo_engine': 'Active - Pattern Matching + Local AI Database',
            'anna_ai': 'Active - Deep Analysis (Local Gemma)',
            'url_checker': 'Active - Google Safe Browsing' if SAFE_BROWSING_API_KEY else 'Inactive',
            'email_alerts': 'Active - Gmail API',
            'history_logging': 'Active - Google Sheets'
        }
    })

@analyze_endpoint.route('/init', methods=['POST'])
async def initialize_model():
    """Endpoint Ä‘á»ƒ khá»Ÿi táº¡o model thá»§ cÃ´ng"""
    if model_state.is_loaded():
        return jsonify({
            'status': 'already_loaded',
            'message': 'Model Ä‘Ã£ Ä‘Æ°á»£c táº£i trÆ°á»›c Ä‘Ã³'
        })
    
    print("ğŸ”„ [Init] Starting manual model initialization...")
    success = load_gemma_model()
    
    if success:
        return jsonify({
            'status': 'success',
            'message': 'Model Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng',
            'model_name': MODEL_NAME,
            'timestamp': datetime.now().isoformat()
        })
    else:
        return jsonify({
            'status': 'failed',
            'message': 'KhÃ´ng thá»ƒ táº£i model',
            'timestamp': datetime.now().isoformat()
        }), 500

@analyze_endpoint.route('/cache/clear', methods=['POST'])
async def clear_cache():
    """Endpoint Ä‘á»ƒ xÃ³a cache thá»§ cÃ´ng"""
    global cached_dangerous_regex, cached_trivial_set, cache_timestamp
    
    cached_dangerous_regex = None
    cached_trivial_set = None
    cache_timestamp = 0
    
    # Clear GPU cache if available
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    gc.collect()
    
    print("ğŸ”„ [Cache] Cache cleared manually")
    return jsonify({
        'status': 'success',
        'message': 'Cache Ä‘Ã£ Ä‘Æ°á»£c xÃ³a thÃ nh cÃ´ng',
        'timestamp': datetime.now().isoformat()
    })

@analyze_endpoint.route('/cache/status', methods=['GET'])
async def cache_status():
    """Endpoint Ä‘á»ƒ kiá»ƒm tra tráº¡ng thÃ¡i cache"""
    now = datetime.now().timestamp() * 1000
    cache_age_ms = now - cache_timestamp if cache_timestamp > 0 else 0
    cache_remaining_ms = max(0, CACHE_DURATION - cache_age_ms)
    
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            'cuda_available': True,
            'device_count': torch.cuda.device_count(),
            'current_device': torch.cuda.current_device(),
            'device_name': torch.cuda.get_device_name(0),
            'memory_allocated_mb': round(torch.cuda.memory_allocated(0) / 1024 / 1024, 2),
            'memory_reserved_mb': round(torch.cuda.memory_reserved(0) / 1024 / 1024, 2)
        }
    else:
        gpu_info = {'cuda_available': False}
    
    return jsonify({
        'pattern_cache': {
            'dangerous_regex_cached': cached_dangerous_regex is not None,
            'trivial_set_cached': cached_trivial_set is not None,
            'cache_age_minutes': round(cache_age_ms / 60000, 2),
            'cache_remaining_minutes': round(cache_remaining_ms / 60000, 2),
            'cache_duration_minutes': CACHE_DURATION / 60000,
            'cache_expired': cache_age_ms >= CACHE_DURATION
        },
        'gpu_info': gpu_info
    })

@analyze_endpoint.route('/stats', methods=['GET'])
async def get_stats():
    """Endpoint Ä‘á»ƒ láº¥y thá»‘ng kÃª há»‡ thá»‘ng"""
    def get_leo_stats():
        try:
            dangerous_count = len(get_sheet_data(DANGEROUS_SHEET_NAME) or [])
            trivial_count = len(get_sheet_data(TRIVIAL_SHEET_NAME) or [])
            hint_count = len(get_sheet_data(HINT_SHEET_NAME) or [])
            
            return {
                'dangerous_patterns': dangerous_count,
                'trivial_patterns': trivial_count,
                'context_hints': hint_count,
                'total_patterns': dangerous_count + trivial_count + hint_count
            }
        except:
            return None
    
    leo_stats = get_leo_stats()
    
    return jsonify({
        'status': 'active' if model_state.is_loaded() else 'model_not_loaded',
        'timestamp': datetime.now(timezone(timedelta(hours=7))).isoformat(),
        'model': {
            'name': MODEL_NAME,
            'loaded': model_state.is_loaded(),
            'cache_dir': CACHE_DIR
        },
        'leo_database': leo_stats or {'error': 'Cannot fetch stats'},
        'cache_info': {
            'dangerous_regex_cached': cached_dangerous_regex is not None,
            'trivial_set_cached': cached_trivial_set is not None,
            'cache_age_minutes': round(((datetime.now().timestamp() * 1000) - cache_timestamp) / 60000, 2) if cache_timestamp > 0 else 0
        }
    })

@analyze_endpoint.route('/test-model', methods=['POST'])
async def test_model():
    """Endpoint Ä‘á»ƒ test model vá»›i input tÃ¹y chá»‰nh"""
    if not model_state.is_loaded():
        return jsonify({'error': 'Model chÆ°a Ä‘Æ°á»£c táº£i'}), 503
    
    data = request.get_json(silent=True)
    if not data or 'prompt' not in data:
        return jsonify({'error': 'Missing prompt field'}), 400
    
    prompt = data.get('prompt', '').strip()
    max_tokens = data.get('max_tokens', 100)
    temperature = data.get('temperature', 0.7)
    
    print(f"ğŸ§ª [Test] Testing model with prompt: '{prompt[:100]}'")
    
    try:
        result = generate_with_gemma(prompt, max_tokens=max_tokens, temperature=temperature)
        return jsonify({
            'status': 'success',
            'prompt': prompt,
            'result': result,
            'params': {
                'max_tokens': max_tokens,
                'temperature': temperature
            }
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500

# =================================================================
# ERROR HANDLERS
# =================================================================

@analyze_endpoint.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'Endpoint khÃ´ng tá»“n táº¡i'}), 404

@analyze_endpoint.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'Lá»—i ná»™i bá»™ server'}), 500

@analyze_endpoint.errorhandler(503)
def service_unavailable(error):
    return jsonify({'error': 'Dá»‹ch vá»¥ táº¡m thá»i khÃ´ng kháº£ dá»¥ng'}), 503

# =================================================================
# STARTUP INITIALIZATION
# =================================================================

def initialize_on_startup():
    """HÃ m khá»Ÿi táº¡o khi server start"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘        CyberShield Analysis System v3.0 (LOCAL)         â•‘
    â•‘              Powered by Local Gemma-3-270M              â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  Components:                                             â•‘
    â•‘  â€¢ Leo Engine (Local Gemma DB-AI)                       â•‘
    â•‘  â€¢ Anna-AI (Local Gemma Deep Analysis)                  â•‘
    â•‘  â€¢ URL Safety Checker                                    â•‘
    â•‘  â€¢ Email Alerts (Gmail API)                             â•‘
    â•‘  â€¢ History Logging (Google Sheets)                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print("\nğŸ” Checking configuration...")
    print(f"âœ“ Model: {MODEL_NAME}")
    print(f"âœ“ Cache Directory: {CACHE_DIR}")
    print(f"âœ“ Safe Browsing API: {'Configured' if SAFE_BROWSING_API_KEY else 'Not configured'}")
    print(f"âœ“ Google Sheet ID: {'Configured' if GOOGLE_SHEET_ID else 'Not configured'}")
    print(f"âœ“ Gmail Token Path: {GMAIL_TOKEN_PATH}")
    print(f"âœ“ Cache Duration: {CACHE_DURATION / 60000} minutes")
    
    print("\nğŸš€ Loading Gemma model...")
    success = load_gemma_model()
    
    if success:
        print("\nâœ… System ready! All components initialized successfully.\n")
    else:
        print("\nâš ï¸ WARNING: Model failed to load. Call /init endpoint to retry.\n")
    
    return success

# Export model_state Ä‘á»ƒ app.py cÃ³ thá»ƒ import
__all__ = ['analyze_endpoint', 'initialize_on_startup', 'model_state']

# =================================================================
# MAIN ENTRY POINT
# =================================================================

if __name__ == '__main__':
    # Initialize on startup
    initialize_on_startup()
    
    print("=" * 60)
    print("Server is ready to accept requests!")
    print("=" * 60)
    print("\nAvailable endpoints:")
    print("  POST /analyze          - Analyze message")
    print("  GET  /health           - Health check")
    print("  POST /init             - Initialize model manually")
    print("  GET  /stats            - System statistics")
    print("  POST /test-model       - Test model with custom prompt")
    print("  POST /cache/clear      - Clear cache")
    print("  GET  /cache/status     - Cache status")
    print("\n" + "=" * 60)